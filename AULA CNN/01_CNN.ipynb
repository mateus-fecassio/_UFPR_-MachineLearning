{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01_CNN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM/2IS89cU2mgHQJtKhOR3g"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"iBXNbvMLPoY3","colab_type":"text"},"source":["---\n","\n","# ***01 - CNN***\n","\n","---\n","\n","**Aprendizagem de Máquina**\n","\n","Gustavo H. G. Matsushita (gustavomatsushita@ufpr.br)\n","\n","Prof. Luiz Eduardo S. Oliveira (luiz.oliveira@ufpr.br)\n","\n","---\n","\n","**Universidade Federal do Paraná**\n","\n","Departamento de Informática\n","\n","http://web.inf.ufpr.br/luizoliveira\n","\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"qPiB7p78mHVZ","colab_type":"text"},"source":["\n","**Keras:**\n","https://keras.io/getting_started/"]},{"cell_type":"markdown","metadata":{"id":"kBGQYdX7ww5Z","colab_type":"text"},"source":["#Importando do Google Drive"]},{"cell_type":"code","metadata":{"id":"NYrRT74AwoIz","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ikhMIDRFx5ow","colab_type":"text"},"source":["#Importando módulos no Python"]},{"cell_type":"code","metadata":{"id":"r5uZAfmgSQ1j","colab_type":"code","colab":{}},"source":["import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","\n","from sklearn.metrics import confusion_matrix\n","\n","from PIL import Image\n","\n","import keras\n","from keras.datasets import mnist\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras import backend as K\n","\n","from keras.applications.inception_v3 import InceptionV3\n","from keras.applications.inception_v3 import preprocess_input\n","from keras.models import Model\n","from keras.layers import GlobalAveragePooling2D\n","from keras.preprocessing import image\n","\n","print('Done')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_A1VYGHzRgks","colab_type":"text"},"source":["#Verificando GPU\n","\n","(Editar > Configurações de Notebook > Acelerador de hardware > **GPU** > Salvar)"]},{"cell_type":"code","metadata":{"id":"HdgULoWDNRwh","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mksTnlljPcCt","colab_type":"code","colab":{}},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lppn1n_3Etbj","colab_type":"text"},"source":["#Definindo algumas variáveis\n","(número de classes, épocas, tamanho dos batch, **arquivos de entrada**...)"]},{"cell_type":"code","metadata":{"id":"DN9-TpiKgEld","colab_type":"code","colab":{}},"source":["## Google Drive\n","drive_path = '/content/drive/My Drive/Colab Notebooks/Aulas/'\n","\n","## Classes\n","num_classes = 12\n","\n","## Batch Size\n","batch_size = 64\n","\n","## Epochs\n","n_epochs = 64\n","\n","## Learning rate\n","learning_rate=0.01\n","\n","## Train and Test files\n","train_file = drive_path + 'train.txt'\n","test_file = drive_path + 'test.txt'\n","\n","## Input Image Dimension\n","img_rows, img_cols = 64, 64\n","\n","print('Done')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GXOChfRKF76Z","colab_type":"text"},"source":["#Funções para ler e preparar a base de dados"]},{"cell_type":"code","metadata":{"id":"Cbwc8aB5gmYs","colab_type":"code","colab":{}},"source":["## Resize\n","\n","def resize_data(data, size, convert):\n","\n","\tif convert:\n","\t\tdata_upscaled = np.zeros((data.shape[0], size[0], size[1], 3))\n","\telse:\n","\t\tdata_upscaled = np.zeros((data.shape[0], size[0], size[1]))\n","\tfor i, img in enumerate(data):\n","\t\tlarge_img = cv2.resize(img, dsize=(size[1], size[0]), interpolation=cv2.INTER_CUBIC)\n","\t\tdata_upscaled[i] = large_img\n","\n","\t#print (np.shape(data_upscaled))\n","\treturn data_upscaled\n","  \n","print('Done')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zwetklxVgK4e","colab_type":"code","colab":{}},"source":["## Load Images\n","\n","def load_images(image_paths, convert=False):\n","\n","\tx = []\n","\ty = []\n","\tfor image_path in image_paths:\n","\n","\t\tpath, label = image_path.split(' ')\n","\t\t\n","\t\t## Image path\n","\t\tpath= drive_path + 'data/' + path\n","\t\tprint (path)\n","\n","\t\tif convert:\n","\t\t\timage_pil = Image.open(path).convert('RGB') \n","\t\telse:\n","\t\t\timage_pil = Image.open(path).convert('L')\n","\n","\t\timg = np.array(image_pil, dtype=np.uint8)\n","\n","\t\tx.append(img)\n","\t\ty.append([int(label)])\n","\n","\tx = np.array(x)\n","\ty = np.array(y)\n","\n","\tif np.min(y) != 0: \n","\t\ty = y-1\n","\n","\treturn x, y\n","\n","print('Done')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3nvT443Phisg","colab_type":"code","colab":{}},"source":["## Load Dataset\n","\n","def load_dataset(train_file, test_file, resize, convert=False, size=(224,224)):\n","\n","\tarq = open(train_file, 'r')\n","\ttexto = arq.read()\n","\ttrain_paths = texto.split('\\n')\n","\t\n","\tprint ('Size:', size)\n","\n","\ttrain_paths.remove('') # Remove empty lines\n","\ttrain_paths.sort()\n","\n","\tprint (\"Loading training set...\")\n","\tx_train, y_train = load_images(train_paths, convert)\n"," \n","\tarq = open(test_file, 'r')\n","\ttexto = arq.read()\n","\ttest_paths = texto.split('\\n')\n","\n","\ttest_paths.remove('') # Remove empty lines\n","\ttest_paths.sort()\n"," \n","\tprint (\"Loading testing set...\")\n","\tx_test, y_test = load_images(test_paths, convert)\n","\n","\tif resize:\n","\t\tprint (\"Resizing images...\")\n","\t\tx_train = resize_data(x_train, size, convert)\n","\t\tx_test = resize_data(x_test, size, convert)\n","\n","\tif not convert:\n","\t\tx_train = x_train.reshape(x_train.shape[0], size[0], size[1], 1)\n","\t\tx_test = x_test.reshape(x_test.shape[0], size[0], size[1], 1)\n","\n","\tprint (np.shape(x_train))\n","\treturn (x_train, y_train), (x_test, y_test)\n"," \n","print('Done')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YLMlXWRlGxwY","colab_type":"text"},"source":["# **1. Carregando as bases de treino e teste**"]},{"cell_type":"code","metadata":{"id":"UVSYip88hlOv","colab_type":"code","colab":{}},"source":["print (\"Loading database...\")\n","\n","## Gray Scale\n","#input_shape = (img_rows, img_cols, 1)\n","#(x_train, y_train), (x_test, y_test) = load_dataset(train_file, test_file, resize=True, convert=False, size=(img_rows, img_cols))\n","\n","## RGB\n","input_shape = (img_rows, img_cols, 3)\n","(x_train, y_train), (x_test, y_test) = load_dataset(train_file, test_file, resize=True, convert=True, size=(img_rows, img_cols))\n","\n","## Save for the confusion matrix\n","label = []\n","for i in range(len(x_test)):\n","\tlabel.append(y_test[i][0])\n","\n","## Normalize images\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","x_train /= 255\n","x_test /= 255\n","#print ('\\n','x_train shape:', x_train.shape)\n","\n","print ('\\n',x_train.shape[0], 'train samples')\n","print ('\\n',x_test.shape[0], 'test samples')\n","\n","\n","## Convert class vectors to binary class matrices\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-QXidk7CIcXG","colab_type":"text"},"source":["# **2. Difinindo o modelo da CNN**"]},{"cell_type":"code","metadata":{"id":"nccYGPYInFUd","colab_type":"code","colab":{}},"source":["## Create CNN model\n","model = Sequential()\n","model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n","model.add(Conv2D(64, (3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","model.add(Flatten()) #gera um vetor de características. Podemos parar aqui e utilizar isso em um outro classificador.\n","\n","# a partir daqui nós estamos chegamos na das camadas totalmente conectadas, onde ocorre a classificação em si.\n","# \n","model.add(Dense(128, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(num_classes, activation='softmax')) #cuidado! Aqui a última saída tem que ser o nosso número de classes e uma função de ativação de softmax (saída é um vetor de probabilidade para cada classe)\n","\n","## Print CNN layers\n","print ('Network structure ----------------------------------')\n","\n","# for i, layer in enumerate(model.layers):\n","# \tprint(i,layer.name)\n","# \tif hasattr(layer, 'output_shape'):\n","# \t\tprint(layer.output_shape)\n","\n","model.summary()\n","\n","print ('----------------------------------------------------')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P0ZBMm0CJkbT","colab_type":"text"},"source":["# **3. Configurando e treinando a CNN**"]},{"cell_type":"code","metadata":{"id":"X8GAzseUtYMw","colab_type":"code","colab":{}},"source":["## Configures the model for training\n","model.compile(metrics=['accuracy'], loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(learning_rate=learning_rate))\n","\n","## Trains the model\n","history = model.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=n_epochs, verbose=1, validation_data=(x_test, y_test))\n","\n","score = model.evaluate(x_test, y_test, verbose=0)\n","print ('\\n----------------------------------------------------\\n')\n","print ('Test loss:', score[0])\n","print ('Test accuracy:', score[1])\n","print ('\\n----------------------------------------------------\\n')\n","\n","## Classes predicted\n","#print (model.predict_classes(x_test)) \n","\n","## Classes probability\n","#print (model.predict_proba(x_test)) \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YX_SmrdxKwQH","colab_type":"text"},"source":["#Matriz de confusão"]},{"cell_type":"code","metadata":{"id":"kS-JQ6OL6K0m","colab_type":"code","colab":{}},"source":["pred = []\n","y_pred = model.predict_classes(x_test)\n","# y_pred = y_prob.argmax(axis=-1)\n","for i in range(len(x_test)):\n","\tpred.append(y_pred[i])\n","print (confusion_matrix(label, pred))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ftryNw3tLhhK","colab_type":"text"},"source":["#Plotando gráficos"]},{"cell_type":"code","metadata":{"id":"PMIEsNUM6TPY","colab_type":"code","colab":{}},"source":["acc = history.history['accuracy'] # history['acc'] / history['accuracy']\n","val_acc = history.history['val_accuracy'] # history['val_acc'] / history['val_accuracy']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n"," \n","epochs = range(len(acc))\n"," \n","plt.plot(epochs, acc, 'b', label='Training acc')\n","plt.plot(epochs, val_acc, 'r', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n"," \n","plt.figure()\n"," \n","plt.plot(epochs, loss, 'b', label='Training loss')\n","plt.plot(epochs, val_loss, 'r', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n"," \n","plt.show()"],"execution_count":null,"outputs":[]}]}